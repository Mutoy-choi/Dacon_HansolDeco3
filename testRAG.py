import os
import torch
import numpy as np
import pandas as pd
from typing import List
import nltk
nltk.download('punkt')
from nltk.tokenize import sent_tokenize

from transformers import AutoTokenizer, AutoModel
from langchain.embeddings.base import Embeddings
from langchain.docstore.in_memory import InMemoryDocstore
from langchain_community.vectorstores import FAISS

# ì»¤ìŠ¤í…€ ì„ë² ë”© í´ë˜ìŠ¤ (HuggingFace Granite ëª¨ë¸ ì‚¬ìš©)
class GraniteEmbeddings(Embeddings):
    def __init__(self, model_name: str = "ibm-granite/granite-embedding-107m-multilingual", device: str = None):
        if device is None:
            device = "cuda" if torch.cuda.is_available() else "cpu"
        self.device = device
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name).to(self.device)
        self.model.eval()
    
    def embed_query(self, text: str) -> List[float]:
        return self._embed(text)
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return [self._embed(text) for text in texts]
    
    def _embed(self, text: str) -> List[float]:
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        with torch.no_grad():
            outputs = self.model(**inputs)
        # ë§ˆì§€ë§‰ hidden stateì˜ í‰ê· ê°’ì„ ì„ë² ë”© ë²¡í„°ë¡œ ì‚¬ìš©
        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()
        return embedding

def cosine_similarity(vec1, vec2):
    vec1 = np.array(vec1)
    vec2 = np.array(vec2)
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2) + 1e-8)

if __name__ == "__main__":
    # ì €ì¥ëœ FAISS ì¸ë±ìŠ¤ ë¡œë“œ (allow_dangerous_deserialization ì˜µì…˜ ì‚¬ìš©)
    index_dir = "backend/faiss_index"
    
    # FAISS DB ë¡œë“œ ì‹œ ì˜¬ë°”ë¥¸ ì–¸íŒ¨í‚¹ ì ìš©
    try:
        vectordb = FAISS.load_local(index_dir, embeddings=GraniteEmbeddings(), allow_dangerous_deserialization=True)
        print("FAISS index loaded successfully.")
    except TypeError as e:
        print(f"FAISS ë¡œë“œ ì˜¤ë¥˜: {e}")
        
        # ê°•ì œ ì–¸íŒ¨í‚¹ ì‹œë„ (v0.1.x ì´í›„ ë²„ì „ì˜ ë³€ê²½ì  ëŒ€ì‘)
        faiss_index, metadata_store = FAISS.load_local(index_dir, GraniteEmbeddings(), allow_dangerous_deserialization=True)

        # `faiss_index`ì™€ `metadata_store`ê°€ ì˜¬ë°”ë¥´ê²Œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
        if isinstance(metadata_store, InMemoryDocstore):
            print("ğŸ“Œ FAISSì™€ ë©”íƒ€ë°ì´í„°ê°€ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
            vectordb = FAISS(index=faiss_index, docstore=metadata_store, embedding_function=GraniteEmbeddings())
        else:
            raise ValueError("âŒ ì˜¬ë°”ë¥¸ FAISS ë©”íƒ€ë°ì´í„° í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")

    # test.csv íŒŒì¼ ë¡œë“œ (ì‚¬ê³  ë°ì´í„°ê°€ CSV í˜•ì‹ìœ¼ë¡œ ì €ì¥ë˜ì–´ ìˆë‹¤ê³  ê°€ì •)
    test_df = pd.read_csv("backend/test.csv")
    
    # ì˜ˆì‹œë¡œ ì²« ë²ˆì§¸ í–‰ì˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ê³ ì›ì¸ ì¿¼ë¦¬ ë¬¸ìì—´ ìƒì„± (ì‚¬ê³ ì›ì¸ ì»¬ëŸ¼ ì‚¬ìš©)
    accident_cause = str(test_df.iloc[0]["ì‚¬ê³ ì›ì¸"]).strip()
    print("Accident Cause Query:", accident_cause)
    
    # 1ë‹¨ê³„: FAISS DBì—ì„œ ì‚¬ê³ ì›ì¸ê³¼ ìœ ì‚¬í•œ ì²­í¬ ê²€ìƒ‰ (ìœ ì‚¬ë„ ì ìˆ˜ì™€ í•¨ê»˜)
    results_with_scores = vectordb.similarity_search_with_score(accident_cause, k=5)
    
    # 2ë‹¨ê³„: ê° ì²­í¬ì˜ metadataì— ì €ì¥ëœ llm_keywordsì™€ ì‚¬ê³ ì›ì¸ ê°„ì˜ ì„ë² ë”© ìœ ì‚¬ë„ ê³„ì‚°
    min_llm_keywords_similarity_threshold = 0.7
    embedding_model = GraniteEmbeddings()
    accident_cause_emb = embedding_model.embed_query(accident_cause)
    
    final_results = []
    for doc, score in results_with_scores:
        llm_keywords = doc.metadata.get("llm_keywords", "")
        if llm_keywords:
            llm_keywords_emb = embedding_model.embed_query(llm_keywords)
            sim = cosine_similarity(accident_cause_emb, llm_keywords_emb)
            if sim >= min_llm_keywords_similarity_threshold:
                final_results.append((doc, score, sim))
        else:
            continue
    
    if not final_results:
        print("ì‚¬ê³ ì›ì¸ê³¼ llm_keywordsì˜ ìœ ì‚¬ë„ê°€ ì„ê³„ì¹˜ ì´ìƒì¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print("\nAccident Cause Query:", accident_cause)
        print("Filtered Documents:")
        for i, (doc, score, llm_sim) in enumerate(final_results):
            title = doc.metadata.get("document_title", "No Title")
            llm_keywords = doc.metadata.get("llm_keywords", "N/A")
            metadata = doc.metadata
            chunk_content = doc.page_content
            print(f"\nDocument {i+1}:")
            print("Title:", title)
            print("FAISS Score:", score)
            print("llm_keywords ìœ ì‚¬ë„:", llm_sim)
            print("LLM Keywords:", llm_keywords)
            print("Metadata:", metadata)
            print("Chunk Content:")
            print(chunk_content)
            print("-" * 40)
