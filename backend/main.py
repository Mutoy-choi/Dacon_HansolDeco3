import os
import torch
import numpy as np
import pandas as pd
from typing import List
import nltk
# nltk.download('punkt')
# nltk.download('punkt_tab')
from nltk.tokenize import sent_tokenize

from fastapi import FastAPI
from pydantic import BaseModel, Field
import ollama
import json
import math
from sklearn.metrics.pairwise import cosine_similarity
from fastapi.middleware.cors import CORSMiddleware
import logging
from fastapi.responses import JSONResponse

# --- ì¶”ê°€: FAISSì™€ LLM ì„ë² ë”© ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ---
from transformers import AutoTokenizer, AutoModel
from langchain.embeddings.base import Embeddings
from langchain.docstore.document import Document
from langchain_community.vectorstores import FAISS

# âœ… ë¡œê·¸ ì„¤ì •
LOG_FILE = "logs.txt"
logging.basicConfig(filename=LOG_FILE, level=logging.INFO, format="%(asctime)s - %(message)s")

# ì»¤ìŠ¤í…€ ì„ë² ë”© í´ë˜ìŠ¤ (HuggingFace Granite ëª¨ë¸ ì‚¬ìš©)
class GraniteEmbeddings(Embeddings):
    def __init__(self, model_name: str = "ibm-granite/granite-embedding-107m-multilingual", device: str = None):
        if device is None:
            device = "cuda" if torch.cuda.is_available() else "cpu"
        self.device = device
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name).to(self.device)
        self.model.eval()
    
    def embed_query(self, text: str) -> List[float]:
        return self._embed(text)
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return [self._embed(text) for text in texts]
    
    def _embed(self, text: str) -> List[float]:
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        with torch.no_grad():
            outputs = self.model(**inputs)
        # ë§ˆì§€ë§‰ hidden stateì˜ í‰ê· ê°’ì„ ì„ë² ë”© ë²¡í„°ë¡œ ì‚¬ìš©
        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()
        return embedding

def log_message(message: str):
    """ë¡œê·¸ ë©”ì‹œì§€ë¥¼ íŒŒì¼ì— ì €ì¥"""
    logging.info(message)

app = FastAPI()

# âœ… CORS ì„¤ì • ì¶”ê°€
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# âœ… í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ íŒŒì¼ ë¡œë“œ (test.csv)
TEST_FILE = "test.csv"
test_data = pd.read_csv(TEST_FILE)

# ì „ì—­ ë³€ìˆ˜ë¡œ í˜„ì¬ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ì˜ ì¸ë±ìŠ¤ë¥¼ ê´€ë¦¬ (ì´ˆê¸°ê°’ 0)
current_test_index = 0

@app.get("/next_test_case")
def next_test_case():
    global current_test_index
    if current_test_index >= len(test_data):
        return JSONResponse(content={"message": "ëª¨ë“  í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ê°€ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤."})
    
    # test.csv íŒŒì¼ì˜ í˜„ì¬ í–‰ ê°€ì ¸ì˜¤ê¸° (ì˜ˆ: ì»¬ëŸ¼ ì´ë¦„ "ì‚¬ê³ ê°ì²´", "ì‚¬ê³ ì›ì¸")
    row = test_data.iloc[current_test_index]
    current_test_index += 1

    return {
        "accident_object": row["ì‚¬ê³ ê°ì²´"],
        "accident_cause": row["ì‚¬ê³ ì›ì¸"],
        "gongjong": row["ê³µì¢…"],
        "jobProcess": row["ì‘ì—…í”„ë¡œì„¸ìŠ¤"],
        "location": row["ì¥ì†Œ"],
        "part": row["ë¶€ìœ„"],
        "humanAccident": row["ì¸ì ì‚¬ê³ "],
        "materialAccident": row["ë¬¼ì ì‚¬ê³ "],

    }

# âœ… API: ë¡œê·¸ ê°€ì ¸ì˜¤ê¸°
@app.get("/logs")
def get_logs():
    """ì €ì¥ëœ ë¡œê·¸ë¥¼ í´ë¼ì´ì–¸íŠ¸ë¡œ ë°˜í™˜"""
    try:
        with open(LOG_FILE, "r", encoding="utf-8") as f:
            logs = f.readlines()
        return JSONResponse(content={"logs": logs[-10:]})  # ìµœê·¼ 10ê°œ ë¡œê·¸ ë°˜í™˜
    except FileNotFoundError:
        return JSONResponse(content={"logs": ["ë¡œê·¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."]})
    
# âœ… NumPy íƒ€ì…ì„ Python ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def convert_to_serializable(obj):
    if isinstance(obj, np.float32) or isinstance(obj, np.float64):
        return float(np.nan_to_num(obj))  # nanì„ 0.0ìœ¼ë¡œ ë³€í™˜
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    return obj

# âœ… NaN ê°’ ì¬ê·€ì  ë³€í™˜ í•¨ìˆ˜
def sanitize_data(data):
    if isinstance(data, dict):
        return {k: sanitize_data(v) for k, v in data.items()}
    elif isinstance(data, list):
        return [sanitize_data(item) for item in data]
    elif isinstance(data, float):
        return 0.0 if math.isnan(data) else data
    else:
        return data

# âœ… ë°ì´í„° ê²½ë¡œ
DATA_FILE = "responses.jsonl"
VECTOR_FILE = "vectorized_data.npz"
DB_FILE = "database.csv"

# âœ… ë°ì´í„° ë¡œë“œ (í•™ìŠµ ë°ì´í„°)
train_data = pd.read_csv(DB_FILE)

# âœ… ìµœì  ê°€ì¤‘ì¹˜ ì„¤ì •
optimal_weights = {
    'ì‚¬ê³ ì›ì¸': 0.45,
    'ê³µì¢…': 0.15,
    'ì‘ì—…í”„ë¡œì„¸ìŠ¤': 0.1,
    'ì¸ì ì‚¬ê³ ': 0.0,
    'ë¬¼ì ì‚¬ê³ ': 0.0,
    'ì¥ì†Œ': 0.05,
    'ì‚¬ê³ ê°ì²´': 0.15,
    'ë¶€ìœ„': 0.1
}
factor_cols = list(optimal_weights.keys())

# âœ… ë²¡í„° ë°ì´í„° ë¡œë“œ ë° ì •ê·œí™” (ê° ìš”ì†Œë³„ ë²¡í„°)
vector_data = np.load(VECTOR_FILE)
vector_dict = {}
for factor in factor_cols:
    if factor == "ì‚¬ê³ ì›ì¸":
        vector_dict[factor] = vector_data["ì‚¬ê³ ì›ì¸"]
    elif factor == "ì‚¬ê³ ê°ì²´":
        vector_dict[factor] = vector_data["ì‚¬ê³ ê°ì²´"]
    else:
        vector_dict[factor] = vector_data[f"{factor}"]
    # L2 ì •ê·œí™” (0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ë°©ì§€)
    vector_norm = np.linalg.norm(vector_dict[factor], axis=1, keepdims=True)
    vector_norm[vector_norm == 0] = 1
    vector_dict[factor] = vector_dict[factor] / vector_norm

# âœ… ëª¨ë¸ ë¡œë“œ (ì‚¬ê³  ìš”ì†Œ ë²¡í„°í™”ì—ëŠ” SentenceTransformer ì‚¬ìš©)
device = "cuda" if torch.cuda.is_available() else "cpu"
from sentence_transformers import SentenceTransformer
model = SentenceTransformer("ibm-granite/granite-embedding-107m-multilingual", device=device)

# ê¸°ì¡´ ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜ (ë ˆê±°ì‹œìš©: ì‚¬ê³ ê°ì²´ì™€ ì‚¬ê³ ì›ì¸ë§Œ ì‚¬ìš©)
def compute_similarity(vector, matrix):
    norm = np.linalg.norm(vector)
    if norm == 0:
        return np.zeros(matrix.shape[0])
    vector = vector / norm
    similarity = np.dot(matrix, vector.T)
    return np.nan_to_num(similarity)

def find_similar_accidents(object_text, cause_text, top_n=5):
    log_message(f"ğŸ” ìœ ì‚¬ ì‚¬ê³  ê²€ìƒ‰ ì‹œì‘: {object_text}, {cause_text}")

    test_object_vector = model.encode([object_text])[0]
    test_cause_vector = model.encode([cause_text])[0]

    object_norm = np.linalg.norm(test_object_vector)
    cause_norm = np.linalg.norm(test_cause_vector)

    if object_norm == 0:
        test_object_vector = np.zeros_like(test_object_vector)
    else:
        test_object_vector /= object_norm

    if cause_norm == 0:
        test_cause_vector = np.zeros_like(test_cause_vector)
    else:
        test_cause_vector /= cause_norm

    object_similarity_test = compute_similarity(test_object_vector, vector_dict["ì‚¬ê³ ê°ì²´"])
    cause_similarity_test = compute_similarity(test_cause_vector, vector_dict["ì‚¬ê³ ì›ì¸"])

    final_similarity_test = (object_similarity_test * optimal_weights["ì‚¬ê³ ê°ì²´"]) + (cause_similarity_test * optimal_weights["ì‚¬ê³ ì›ì¸"])
    final_similarity_test = np.nan_to_num(final_similarity_test)

    top_indices = np.argsort(-final_similarity_test)[:top_n]
    similar_cases = [
        {
            "case": sanitize_data(train_data.iloc[i].to_dict()),
            "similarity": convert_to_serializable(final_similarity_test[i]),
        }
        for i in top_indices
    ]

    log_message(f"âœ… ìœ ì‚¬ ì‚¬ê³  ê²€ìƒ‰ ì™„ë£Œ: {similar_cases}")

    return similar_cases

# --- 3. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì™€ í•™ìŠµ ë°ì´í„° ê°„ ìœ ì‚¬ë„ ê³„ì‚° ---
# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë²¡í„°í™” (ê° ìš”ì†Œë³„)
test_df = test_data  # test.csv íŒŒì¼ì— factor_colsì— í•´ë‹¹í•˜ëŠ” ì»¬ëŸ¼ì´ ìˆë‹¤ê³  ê°€ì •
test_vector_dict = {
    col: torch.tensor(
        model.encode(test_df[col].tolist(), convert_to_numpy=True),
        device=device,
        dtype=torch.float32
    )
    for col in factor_cols
}

similarity_matrices_test = {}
for col in factor_cols:
    # cosine_similarityëŠ” numpy ë°°ì—´ì„ ìš”êµ¬í•˜ë¯€ë¡œ GPU í…ì„œë¥¼ CPUë¡œ ì´ë™ í›„ ë³€í™˜
    similarity_matrices_test[col] = cosine_similarity(
        test_vector_dict[col].cpu().numpy(),
        vector_dict[col]
    )

# --- 4. ìµœì  ê°€ì¤‘ì¹˜ë¥¼ ë°˜ì˜í•˜ì—¬ ìµœì¢… ìœ ì‚¬ë„ í–‰ë ¬ ìƒì„± ---
final_similarity_test = np.zeros_like(list(similarity_matrices_test.values())[0])
for col in factor_cols:
    final_similarity_test += optimal_weights[col] * similarity_matrices_test[col]

# --- 5. ìœ ì‚¬ ì‚¬ê³  ì°¾ê¸° í•¨ìˆ˜ (í…ŒìŠ¤íŠ¸ ë°ì´í„° ê¸°ì¤€) ---
def find_similar_accidents_for_test(test_index, top_n=5):
    """
    ì£¼ì–´ì§„ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ(test_index)ì˜ ì—¬ëŸ¬ ìš”ì†Œë¥¼ ê³ ë ¤í•˜ì—¬,
    í•™ìŠµ ë°ì´í„°(train_data)ì—ì„œ ê°€ì¥ ìœ ì‚¬í•œ ì‚¬ë¡€ top_nê°œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    similarities = final_similarity_test[test_index]
    top_n = min(top_n, len(similarities))
    similar_indices = np.argsort(-similarities)[:top_n]
    return similar_indices, similarities[similar_indices]

# --- ì¤‘êµ­ì–´ ê°ì§€ ìµœì í™” í•¨ìˆ˜ ---
from langdetect import detect
def is_chinese(text):
    try:
        return detect(text) == "zh-cn"  # ì¤‘êµ­ì–´ ê°ì§€
    except:
        return False  # ê°ì§€ ì‹¤íŒ¨ ì‹œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬

# --- ì—…ë°ì´íŠ¸ëœ Ollama ìš”ì²­ í•¨ìˆ˜ ë° í”„ë¡¬í”„íŠ¸ ---
def generate_rag_response(top_k_cases, ê¸°ì¤€_ì‚¬ê³ ê°ì²´, ê¸°ì¤€_ì‚¬ê³ ì›ì¸, ê¸°ì¤€_ê³µì¢…, ê¸°ì¤€_ì‘ì—…í”„ë¡œì„¸ìŠ¤, ê¸°ì¤€_ì¥ì†Œ, ê¸°ì¤€_ë¶€ìœ„, ê¸°ì¤€_ì¸ì ì‚¬ê³ , ê¸°ì¤€_ë¬¼ì ì‚¬ê³ ):
    system_message = (
        "<|im_start|>system\n"
        "ë‹¹ì‹ ì€ í•œêµ­ì¸ ê±´ì„¤ ì‚¬ê³  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n"
        "ì£¼ì–´ì§„ ìœ ì‚¬ ì‚¬ê³  ì‚¬ë¡€ë“¤ì„ ì°¸ê³ í•˜ì—¬, ê°€ì¥ íš¨ê³¼ì ì¸ ì¬ë°œë°©ì§€ëŒ€ì±… ë° í–¥í›„ ì¡°ì¹˜ ê³„íšì„ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.\n"
        "ì‚¬ê³ ê°ì²´ì™€ ì‚¬ê³ ì›ì¸ì´ ê°€ì¥ ì¤‘ìš”í•œ ìš”ì†Œì´ë©°, ì¸ì ì‚¬ê³ ì™€ ë¬¼ì ì‚¬ê³ ë„ ì¤‘ìš”í•œ ê³ ë ¤ ìš”ì†Œì…ë‹ˆë‹¤.\n"
        "ê³µì¢…, ì‘ì—…í”„ë¡œì„¸ìŠ¤, ì¥ì†Œ, ë¶€ìœ„ë„ ë°˜ì˜í•˜ì—¬ ìµœì ì˜ ëŒ€ì‘ì±…ì„ ë„ì¶œí•˜ì„¸ìš”.\n"
        "ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì•¼ í•˜ë©°, í•œ ë¬¸ì¥ ë‚´ì—ì„œ ê° ìš”ì†Œë¥¼ ë°˜ì˜í•˜ì—¬ 50ë‹¨ì–´ ì´ë‚´ë¡œ ìš”ì•½í•˜ì„¸ìš”.\n"
        "ì¶œë ¥ í˜•ì‹: 'ëŒ€ì‘ëŒ€ì±…:'ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n"
        "<|im_end|>"
    )

    user_message = (
        "<|im_start|>user\n"
        "[ê¸°ì¤€ ì‚¬ê³ ]\n"
        f"- ì‚¬ê³ ê°ì²´: {ê¸°ì¤€_ì‚¬ê³ ê°ì²´}\n"
        f"- ì‚¬ê³ ì›ì¸: {ê¸°ì¤€_ì‚¬ê³ ì›ì¸}\n"
        f"- ì¸ì ì‚¬ê³ : {ê¸°ì¤€_ì¸ì ì‚¬ê³ }\n"
        f"- ë¬¼ì ì‚¬ê³ : {ê¸°ì¤€_ë¬¼ì ì‚¬ê³ }\n"
        f"- ê³µì¢…: {ê¸°ì¤€_ê³µì¢…}\n"
        f"- ì‘ì—…í”„ë¡œì„¸ìŠ¤: {ê¸°ì¤€_ì‘ì—…í”„ë¡œì„¸ìŠ¤}\n"
        f"- ì¥ì†Œ: {ê¸°ì¤€_ì¥ì†Œ}\n"
        f"- ë¶€ìœ„: {ê¸°ì¤€_ë¶€ìœ„}\n\n"
        "[ìœ ì‚¬ ì‚¬ë¡€ - ê¸°ì¡´ ëŒ€ì‘ ëŒ€ì±…ë“¤]:\n" +
        "\n".join([
            f"{i+1}. ì‚¬ê³ ê°ì²´: {case['ì‚¬ê³ ê°ì²´']}, ì‚¬ê³ ì›ì¸: {case['ì‚¬ê³ ì›ì¸']}, ì¸ì ì‚¬ê³ : {case['ì¸ì ì‚¬ê³ ']}, ë¬¼ì ì‚¬ê³ : {case['ë¬¼ì ì‚¬ê³ ']}, ê³µì¢…: {case['ê³µì¢…']}, "
            f"ì‘ì—…í”„ë¡œì„¸ìŠ¤: {case['ì‘ì—…í”„ë¡œì„¸ìŠ¤']}, ì¥ì†Œ: {case['ì¥ì†Œ']}, ë¶€ìœ„: {case['ë¶€ìœ„']}, ëŒ€ì‘ ëŒ€ì±…: {case['ì¬ë°œë°©ì§€ëŒ€ì±… ë° í–¥í›„ì¡°ì¹˜ê³„íš']}"
            for i, case in enumerate(top_k_cases)
        ]) + "\n\n"
        "ìœ„ì˜ ìœ ì‚¬ ì‚¬ë¡€ë“¤ì„ ì°¸ê³ í•˜ì—¬, ê¸°ì¤€ ì‚¬ê³ ì— ëŒ€í•œ ìµœì ì˜ ì¬ë°œë°©ì§€ëŒ€ì±… ë° í–¥í›„ ì¡°ì¹˜ ê³„íšì„ ì‘ì„±í•˜ì„¸ìš”.\n"
        "ìœ ì‚¬ ì‚¬ë¡€ê°€ ë¶€ì¡±í•œ ê²½ìš°, ê¸°ì¡´ ëŒ€ì‘ ëŒ€ì±…ì„ ì¼ë°˜í™”í•˜ì—¬ ìµœì ì˜ í•´ê²°ì±…ì„ ë„ì¶œí•˜ì„¸ìš”.\n"
        "ì¶œë ¥ í˜•ì‹: ë°˜ë“œì‹œ 'ëŒ€ì‘ëŒ€ì±…:'ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n"
        "<|im_end|>"
    )

    messages = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": user_message}
    ]
    
    while True:
        response = ollama.chat(model="qwq", messages=messages)
        output = response["message"]["content"]

        return output


# âœ… API ìš”ì²­ ëª¨ë¸ (ì—…ë°ì´íŠ¸ëœ ì‚¬ê³  ì…ë ¥)
class QueryRequest(BaseModel):
    ì‚¬ê³ ê°ì²´: str
    ì‚¬ê³ ì›ì¸: str
    ê³µì¢…: str
    ì‘ì—…í”„ë¡œì„¸ìŠ¤: str
    ì¥ì†Œ: str
    ë¶€ìœ„: str
    ì¸ì ì‚¬ê³ : str
    ë¬¼ì ì‚¬ê³ : str


# âœ… API: ì‚¬ê³  ì…ë ¥ì„ ë°›ì•„ LLM ì‘ë‹µ ìƒì„± (ì—…ë°ì´íŠ¸ë¨)
@app.post("/generate_responses")
def generate_responses(request: QueryRequest):
    # ì…ë ¥ ë°›ì€ ë°ì´í„° ë¡œê·¸ ê¸°ë¡
    log_message(f"LLM ìš”ì²­ ìˆ˜ì‹ : {request.dict()}")
    
    # ê° ìš”ì†Œë³„ query ë²¡í„° ìƒì„± (ì •ê·œí™” í¬í•¨)
    query_vectors = {}
    for factor in optimal_weights.keys():
        text = getattr(request, factor)
        vec = model.encode(text)
        norm = np.linalg.norm(vec)
        if norm != 0:
            vec /= norm
        query_vectors[factor] = vec

    # ê° ìš”ì†Œë³„ ìœ ì‚¬ë„ ê³„ì‚° í›„ ê°€ì¤‘ì¹˜ ë°˜ì˜í•˜ì—¬ ìµœì¢… ìœ ì‚¬ë„ ì‚°ì¶œ
    similarities = np.zeros(train_data.shape[0])
    for factor in optimal_weights.keys():
        sim = np.dot(vector_dict[factor], query_vectors[factor].T)
        similarities += optimal_weights[factor] * sim

    top_n = 5
    top_indices = np.argsort(-similarities)[:top_n]
    
    # ê° ì¼€ì´ìŠ¤ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ êµ¬ì„± (ìœ ì‚¬ë„ë„ í¬í•¨)
    top_k_cases = [
        {
            "ì‚¬ê³ ê°ì²´": train_data.iloc[i]["ì‚¬ê³ ê°ì²´"],
            "ì‚¬ê³ ì›ì¸": train_data.iloc[i]["ì‚¬ê³ ì›ì¸"],
            "ì¸ì ì‚¬ê³ ": train_data.iloc[i]["ì¸ì ì‚¬ê³ "],
            "ë¬¼ì ì‚¬ê³ ": train_data.iloc[i]["ë¬¼ì ì‚¬ê³ "],
            "ê³µì¢…": train_data.iloc[i]["ê³µì¢…"],
            "ì‘ì—…í”„ë¡œì„¸ìŠ¤": train_data.iloc[i]["ì‘ì—…í”„ë¡œì„¸ìŠ¤"],
            "ì¥ì†Œ": train_data.iloc[i]["ì¥ì†Œ"],
            "ë¶€ìœ„": train_data.iloc[i]["ë¶€ìœ„"],
            "ì¬ë°œë°©ì§€ëŒ€ì±… ë° í–¥í›„ì¡°ì¹˜ê³„íš": train_data.iloc[i]["ì¬ë°œë°©ì§€ëŒ€ì±… ë° í–¥í›„ì¡°ì¹˜ê³„íš"],
            "similarity": float(similarities[i]),
        }
        for i in top_indices
    ]
    log_message(f"ìœ ì‚¬ ì‚¬ê³  ì¼€ì´ìŠ¤ ë„ì¶œ: {top_k_cases}")

    response = generate_rag_response(
        top_k_cases,
        ê¸°ì¤€_ì‚¬ê³ ê°ì²´=request.ì‚¬ê³ ê°ì²´,
        ê¸°ì¤€_ì‚¬ê³ ì›ì¸=request.ì‚¬ê³ ì›ì¸,
        ê¸°ì¤€_ê³µì¢…=request.ê³µì¢…,
        ê¸°ì¤€_ì‘ì—…í”„ë¡œì„¸ìŠ¤=request.ì‘ì—…í”„ë¡œì„¸ìŠ¤,
        ê¸°ì¤€_ì¥ì†Œ=request.ì¥ì†Œ,
        ê¸°ì¤€_ë¶€ìœ„=request.ë¶€ìœ„,
        ê¸°ì¤€_ì¸ì ì‚¬ê³ =request.ì¸ì ì‚¬ê³ ,
        ê¸°ì¤€_ë¬¼ì ì‚¬ê³ =request.ë¬¼ì ì‚¬ê³ 
    )
    
    log_message(f"LLM ì‘ë‹µ ìƒì„± ì™„ë£Œ: {response}")
    
    return {
        "query": {
            "ì‚¬ê³ ê°ì²´": request.ì‚¬ê³ ê°ì²´,
            "ì‚¬ê³ ì›ì¸": request.ì‚¬ê³ ì›ì¸,
            "ê³µì¢…": request.ê³µì¢…,
            "ì‘ì—…í”„ë¡œì„¸ìŠ¤": request.ì‘ì—…í”„ë¡œì„¸ìŠ¤,
            "ì¥ì†Œ": request.ì¥ì†Œ,
            "ë¶€ìœ„": request.ë¶€ìœ„,
            "ì¸ì ì‚¬ê³ ": request.ì¸ì ì‚¬ê³ ,
            "ë¬¼ì ì‚¬ê³ ": request.ë¬¼ì ì‚¬ê³ 
        },
        "top_cases": top_k_cases,
        "answer": response,
    }


# âœ… API ìš”ì²­ ëª¨ë¸ (í”¼ë“œë°± ì €ì¥)
class FeedbackRequest(BaseModel):
    query: str
    winner: str
    loser: str

# âœ… API: í”¼ë“œë°± ì €ì¥
@app.post("/save_feedback")
def save_feedback(request: FeedbackRequest):
    feedback_data = {"query": request.query, "winner": request.winner, "loser": request.loser}
    with open(DATA_FILE, "a", encoding="utf-8") as f:
        json.dump(feedback_data, f, ensure_ascii=False)
        f.write("\n")
    return {"message": "Feedback saved successfully!"}

# --- ì¶”ê°€: FAISS ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ API ---
@app.get("/get_documents")
def get_documents(accident_cause: str = None):
    if not accident_cause:
        accident_cause = str(test_data.iloc[0]["ì‚¬ê³ ì›ì¸"]).strip()
    log_message(f"ë¬¸ì„œ ê²€ìƒ‰ ìš”ì²­: ì‚¬ê³ ì›ì¸={accident_cause}")
    
    try:
        embeddings = GraniteEmbeddings()
        vectordb = FAISS.load_local("faiss_index", embeddings=embeddings, allow_dangerous_deserialization=True)
    except Exception as e:
        log_message(f"FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return JSONResponse(content={"error": f"FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}"})
    
    results_with_scores = vectordb.similarity_search_with_score(accident_cause, k=5)
    min_llm_keywords_similarity_threshold = 0.65
    accident_cause_emb = embeddings.embed_query(accident_cause)
    
    final_results = []
    for doc, score in results_with_scores:
        llm_keywords = doc.metadata.get("llm_keywords", "")
        if llm_keywords:
            llm_keywords_emb = embeddings.embed_query(llm_keywords)
            sim = np.dot(np.array(accident_cause_emb), np.array(llm_keywords_emb)) / (
                (np.linalg.norm(accident_cause_emb) * np.linalg.norm(llm_keywords_emb)) + 1e-8)
            if sim >= min_llm_keywords_similarity_threshold:
                result_dict = {
                    "title": doc.metadata.get("document_title", "No Title"),
                    "faiss_score": float(score),
                    "llm_keywords_similarity": float(sim),
                    "llm_keywords": llm_keywords,
                    "metadata": doc.metadata,
                    "chunk_content": doc.page_content
                }
                final_results.append(result_dict)
    if not final_results:
        log_message("ë¬¸ì„œ ê²€ìƒ‰: ìœ ì‚¬ë„ ì„ê³„ì¹˜ë¥¼ ë§Œì¡±í•˜ëŠ” ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í•¨")
        return {"message": "ìœ ì‚¬ë„ ì„ê³„ì¹˜ë¥¼ ë§Œì¡±í•˜ëŠ” ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."}
    
    log_message(f"ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ: {len(final_results)}ê°œ ë¬¸ì„œ ë°˜í™˜")
    return {
        "accident_cause": accident_cause,
        "documents": final_results
    }

@app.get("/")
def read_root():
    return {"message": "LLM Feedback System is running!"}
